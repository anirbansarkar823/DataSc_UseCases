Approach and Feature Engineering:
    1. Used kaggle for analysis as my personal pc was slow. I have used dataset available on https://www.kaggle.com/imsparsh/churn-risk-rate-hackerearth-ml
    
    2. Then found out that we only need 5 'churn_risk_score' i.e., 1,2,3,4,5 but there was one additional score i.e., -1. Thus removed all rows with 'churn_risk_score' as -1
    
    3. Used train_test_split form sklearn.model_selection to split the data in the beginning itself in train and test set to avoid any data leakage
    
    4. Identified various columns like 'referral_id', 'name' and 'security_no' which as unique for almost all customers thus contributing less in the prediction of target variable 'churn_risk_score'. Thus I dropped them.
    
    5. Then started my EDA, by visualising and using pd.crosstab to find relationship of variables with target variable 'churn_risk_score'. I then realised there were various wrong values inserted as below:
        a. I observed an outlier in variable 'days_since_last_login' i.e., -999. This is as per my logic doesn't suits days (as days cannot be negative). So I replaced all negative values with mean of variable's ('days_since_last_login') mean (mean was taken excluding the negative values). I have updated train, validation and test dataset 
    
        b. I observed variable 'avg_time_spent' was also holding negative values. Thus replaced all the negative values with their absolute values (i.e., -10 changed to 10)
    
        c. I observed variable 'avg_frequency_login_days' was having datatype 'object' which shouldn't be the case. On deeper dive I observed the variable was holding a value 'Error' which was causing this issue. Then there were some negative values too. So, we replaced negative values with their absolute values and then taken mean (excluding 'Error' holding rows). Finally updated all negative and 'Error' value holding rows with the mean value. The updates were done on test, train and validation sets.
    
        d. Found that variable 'joining_date' was a date data, but was represented as string. So I transformed them to corresponding numeric value (day, month and year) and added them to 3 new columns but removed the variable 'joining_date' from train, validation and test set.
    
        e. Found variable 'last_visit_time' was also a time data, but as represented as string. So I transformed them to corresponding numeric values (hour, minute and seconds) and added them to 3 new columns but removed the variable 'last_visit_time' from train, validation and test set. 
    
        f. Variable 'feedback' is the most interesting and holds huge amount of information for predicting our target variable. I referred internet and realised the best would be to apply NLP concepts on this variable. 
            i. removed stopwords and unnecessary characters from each feedback
            
            ii. Used WordNetLemmatizer to convert words from each feedback to their base words as we need to find similarity among feedbacks
            
            iii. Then I found there were many feedbacks which holds same feedback keywords (base words).
            
            iv. there were in total 9 keys unique combination of feedback keywords
            
            v. So with some more internet research I found we can direclty replace those unique feedbacks with numbers ( a variant of LabelEncoding ). 
            
            vi. As future improvement, I thought of using sentiment analysis, to replace values as Positive or negative in place of sentiments. As that would be of great help in predicting the target variable
    
        g. Then with the help of pipeline and ColumnTransformer from Sklearn I performed imputation and encoding on numeric and categorical columns.
    
        f. I have used 3 different kinds of models on train and validation set. These are XGBClassifier, CatBoostClassifier and LGBMClassifier. Out of them XGBClassifier scored highest on validation data.
    
        h. Thus I have chosen XGBClassifier as my final model and used it on test data for submission
Tools:
    1. numpy: for numerical analysis like .unique(), .nunique()
    
    2. pandas: for importing csv to dataframe and doing Exploratory data analysis
    
    3. matplotlib, seaborn: for visualisation like countplot, boxplots, scatter plots
    
    4. warnings: to supress unwanted warnings
    
    5. kaggle dataset: same data which was available, but worked on kaggle
    
    6. sklearn.model_selection: for train_test_split
    
    7. nltk.stem.WordNetLemmatizer: for lemmatizing words to their correct base word. Lemmatizer is much better than stemming
    
    8. nltk.corpus.stopwords: to remove stopwords from feedback
    
    9. re: regular expressions to remove unnecessary characters
    
    10. sklearn.compose.ColumnTransformer: to perform imputation on numerical columns and imputation followed by Encoding for categorical columns
    
    11. sklearn.pipeline: to form pipeline for imputation, encoding and model fitting
    
    12. sklearn.preprocessing.OneHotEncoder: for encoding of categorical columns
    
    13. sklearn.impute.SimpleImputer: for filling missing values with mean/median/most_frequent (mode)
    
    14. xgboost.XGBClassifer: for classification.
    
    15. catboost.CatBoostClassifer: for classification.
    
    16. lightgbm.LGBMClassifier: for classification.
